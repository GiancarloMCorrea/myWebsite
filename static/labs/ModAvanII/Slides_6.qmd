---
title: "Modelos avanzados en evaluación de recursos pesqueros: Día 6"
author: 
   <br>Dr. Giancarlo M. Correa
institute:
   <br>Cousteau Consultant Group
format:
  revealjs:
    theme: simple
    logo: images/CGlogo1.png
    slide-number: 'c/t'
    css: myStyle.css
from: markdown+emoji
editor: visual
execute:
  echo: true
bibliography: "https://api.citedrive.com/bib/b63ac242-a77c-4816-9c27-fcdfb31875b0/references.bib?x=eyJpZCI6ICJiNjNhYzI0Mi1hNzdjLTQ4MTYtOWMyNy1mY2RmYjMxODc1YjAiLCAidXNlciI6ICI1NDQwIiwgInNpZ25hdHVyZSI6ICIyMTMxYjVhZDk1NGYxNDUzYjk3YjZiNjk3ZDIxZWEwODIwZjA4MTBiZTk3YWIxOTNiMGVkMzI1YzI0YTRjZWNkIn0=/bibliography.bib"
---

# WHAM: Valores esperados

## Introducción

::: incremental

- Nos referimos a cantidades que el modelo predice y son contrastadas con observaciones. Por ejemplo: captura agregada predicha vs. captura agregada observada.
- Ya hemos ido viendo algunas variables:
  * Abundancia a la edad
  * Talla media a la edad
  * Peso medio a la edad

:::

## Captura a la edad y talla

<br>

Se calcula usando [@Correa_2023]:

$$\hat{C}_{y,f,l,a} = \varphi_{y,l,a}S_{y,f,l}S_{y,f,a}F_{y,f}*N_{y,a}\frac{1-exp(-Z_{y,a})}{Z_{y,a}}$$

Donde $f$ representa las pesquerías y $Z_{y,a}$ es calculando usando $F$ acumulado (para diferentes pesquerías). 

## Composición por talla y edades

Primero calculamos la sumatoria a lo largo de edades o tallas:

$$\hat{C}_{y,f,a} = \sum_l{\hat{C}_{y,f,l,a}}\hspace{2cm}\hat{C}_{y,f,l} = \sum_a{\hat{C}_{y,f,l,a}}$$

::: fragment

Luego, para calcular la composición marginal (proporción):

$$\hat{p}_{y,f,a}=\frac{\hat{C}_{y,f,a}}{\sum_{a}\hat{C}_{y,f,a}}\hspace{2cm}\hat{p}_{y,f,l}=\frac{\hat{C}_{y,f,l}}{\sum_{l}\hat{C}_{y,f,l}}$$

:::

## Captura agregada

<br>

Lo calculamos:

$$\hat{C}_{y,f}=\sum_a W_{y,a}\hat{C}_{y,f,a}$$

Donde $W_{y,a}$ son los datos de peso a la edad que corresponde a la pesquería $f$ (fracción del año).

## Indice de abundancia

<br>

Se calcula usando:

$$\hat{I}_{y,i,l,a}=\varphi_{y,l,a}S_{y,i,l}S_{y,i,a}N_{a,y}exp(-f_{y,i}Z_{a,y})$$

Donde $i$ indica el índice y $f_{y,i}$ la fracción del año cuando el índice ocurre.

## Indice de abundancia

<br>

Luego calculamos la sumatoria a lo largo de edades o tallas:

$$\hat{I}_{y,i,a} = \sum_l{\hat{I}_{y,i,l,a}}\hspace{2cm}\hat{I}_{y,i,l} = \sum_a{\hat{I}_{y,i,l,a}}$$

::: fragment

Luego, para calcular la composición marginal (proporción):

$$\hat{p}_{y,i,a}=\frac{\hat{I}_{y,i,a}}{\sum_{a}\hat{I}_{y,i,a}}\hspace{2cm}\hat{p}_{y,i,l}=\frac{\hat{I}_{y,i,l}}{\sum_{l}\hat{I}_{y,i,l}}$$

:::


## Indice de abundancia

<br>

Para el índice agregado (en peso):

$$\hat{I}_{y,i} = Q_{y,i}\sum_a W_{y,a}\hat{I}_{y,i,a}$$

Donde $Q$ es la capturabilidad y $W_{y,a}$ es el peso a la edad correspondiente a esa fracción del año.

Para el caso de índice agregado en abundancia, simplemente se omite $W_{y,a}$.

# WHAM: Aspectos estadísticos

## Componentes de verosimilitud

<br>

1. Captura agregada:

Se asume una distribución normal de errores.

<br>

2. Indices de abundancia agregados:

Se asume una distribución normal de errores.

## Componentes de verosimilitud

<br>

3. Peso a la edad:

Se asume una distribución normal de errores.

<br>

4. Variables ambientales:

Se asume una distribución normal de errores.

## Componentes de verosimilitud

<br>

5. Composición a la edad y CAAL:

Tenemos varias opciones y lo podemos especificar:

```{r eval = FALSE}
my_input = prepare_wham_input(...,
                              age_comp = ..., # (character) Definir la distribucion de error
                              ...)
```

<br>

Se recomienda explorar @Fisch_2021.

## Componentes de verosimilitud

<br>

- `"multinomial"`: Opcion por defecto. 0 parámetros.
- `"dir-mult"`: Dirichlet saturante. 1 parámetros.
- `"dirichlet-pool0"`: Se agregan cantidades predecidas cuando hay ceros en observaciones [@Francis_2014]. 1 parámetro.
- `"dirichlet-miss0"`: Trata los ceros en observaciones como valores faltantes. 1 parámetro.

## Componentes de verosimilitud

<br>

- `"logistic-normal-miss0"`: Logistic normal, trata valores ceros como faltantes [@Francis_2014]. 1 parámetro.
- `"logistic-normal-ar1-miss0"`: Logistic normal, trata valores ceros como faltantes e incluye correlación temporal. 2 parámetros.
- `"logistic-normal-pool0"`: Logistic normal, se hace la agregación como en dirichlet. 1 parámetros.

## Componentes de verosimilitud

<br>

- `"logistic-normal-01-infl"`: Cero o uno inflado logistic normal [@Ospina_2012]. 3 parámetros.
- `"mvtweedie"`: Multivariate-tweedie [@Thorson_2022]. 2 parámetros.
- `"dir-mult-linear"`: Linear Dirichlet multinomial [@Thorson_2017]. 1 parámetro.

## Componentes de verosimilitud

<br>

Resumen [@Francis_2014]:

<br>

![](images/D6_img1.png){fig-align="center" width="100%"}

## Componentes de verosimilitud

<br>

6. Composición a la talla:

Tenemos algunas opciones y lo podemos especificar:

```{r eval = FALSE}
my_input = prepare_wham_input(...,
                              len_comp = ..., # (character) Definir la distribucion de error
                              ...)
```

## Componentes de verosimilitud

<br>

- `"multinomial"`: Multinomial. 0 parámetros.
- `"dir-mult"`: Dirichlet saturante. 1 parámetros.
- `"dir-mult-linear"`: Linear Dirichlet multinomial [@Thorson_2017]. 1 parámetro.

## Componentes de verosimilitud

<br>

<br>

::: {.callout-note}

## No olvidar

Dado que WHAM puede ser catalogado como un modelo con enfoque integrado, todas estos componentes de verosimilitud son agregados en un solo componente, el cual será minimizado por el algoritmo de optimización. 

:::


# WHAM: Aproximación de Laplace

## Definición

<br>

El algoritmo de opmitización de TMB es el mismo que para ADMB. Ambas plataformas darán el mismo resultado para un mismo modelo. 

::: fragment

Lo novedoso de TMB es que es mucho más rápido para modelos con efectos aleatorios [@Kristensen_2016]. 

:::

## Definición

'Join likelihood' función, la cual junta los componentes de observación y *estados* (todo el SSM) [@Aeberhard_2018]:

$$L_{joint}(\theta, y_{1:T},x_{0:t}) = p_\theta (x_0) \prod^T_{t=1} p_\theta (y_t \mid x_t)p_\theta (x_t \mid x_{t-1})$$

::: fragment

Sin embargo, lo visto anteriormente no es práctico. Por lo tanto, en ciencias pesqueras se suele utilizar dos enfoques para esto: frecuentista y bayesiano.

:::


## Enfoque frecuentista

Marginal likelihood:

$$L_{marginal}(\theta , y_{1:T}) = \int L_{joint}(\theta, y_{1:T}, x_{0:T})dx_{1:T}$$

::: fragment

Luego de integrar, the maximum likelihood estimator (MLE) para $\theta$:

$$\hat{\theta}_{ML} = \arg \max_{\theta \in \Theta} logL_{marginal}(\theta,y_{1:T})$$

:::

## Enfoque frecuentista

<br>

Ahora, se necesitará alguna forma para integrar (o aproximar) el marginal likelihood. Es justamente aquí donde se utiliza la aproximación de Laplace, realizado de una manera eficiente en TMB. 

::: fragment

Característica importante:

* El marginal likelihood debe ser cercano a una función unimodal (para ser aproximada mediante una función de densidad normal).

:::

# WHAM: Optimización

## Overview

Objetivo: reducir las discrepancias.

![](images/D6_img3.png){fig-align="center" width="100%"}

::: {.absolute top="650" left="70"}
::: sectionhead
[[@Subbey_2018]]{style="font-size:50%"}
:::
:::

## Gradient descent

¿Cómo?: Encontrar el mínimo:

![](images/D6_img4.png){fig-align="center" width="100%"}

::: {.absolute top="650" left="70"}
::: sectionhead
[[@Subbey_2018]]{style="font-size:50%"}
:::
:::

## Gradient descent

Pero no es tan sencillo:

![](images/D6_img7.png){fig-align="center" width="100%"}

::: {.absolute top="650" left="70"}
::: sectionhead
[[@Subbey_2018]]{style="font-size:50%"}
:::
:::

## Gradient descent

Superficie depende de varios factores:

![](images/D6_img5.png){fig-align="center" width="100%"}

::: {.absolute top="650" left="70"}
::: sectionhead
[[@Subbey_2018]]{style="font-size:50%"}
:::
:::

## Gradient descent

Mínimo difícil de encontrar por diversos motivos:

![](images/D6_img6.png){fig-align="center" width="100%"}

::: {.absolute top="650" left="70"}
::: sectionhead
[[@Subbey_2018]]{style="font-size:50%"}
:::
:::

# WHAM: One-step ahead (OSA) residuals 

## Definición

<br>

Usualmente para analizar *datos de composición* utilizamos los Pearson residuals. Sin embargo, cuando tenemos este tipo de datos, las observaciones no son independientes y pueden verse distribuidas diferente a la normal.

<br>

::: fragment

Los OSA residuals aparecen para lidiar con este problema, los cuales de-correlacionan datos de composición cuando estimamos residuos estandarizados [@Trijoulet_2023].

:::

## Ejemplo

![](images/D6_img2.png){fig-align="center" width="100%"}

::: {.absolute top="650" left="70"}
::: sectionhead
[@Trijoulet_2023]{style="font-size:50%"}
:::
:::

## Definición

<br>

Los OSA residuals tiene las siguientes características:

- Independientes
- Normalmente distribuidos
- Media cero
- Varianza igual a uno

## Definición

<br>

Utilizado para las siguientes distribuciones de error (datos de composición):

- Multinomial
- Dirichlet-multinomial
- Logistic-normal


# WHAM: Resultados

<!-- Install this version of knitr or update RStudio-->

<!-- remotes::install_version("knitr", "1.42") -->

## Resultados

<br>

Hay ciertos indicadores que siempre nos interesan explorar. Primero debemos correr el modelo:

```{r eval = FALSE}
my_model = fit_wham(input = my_input, do.retro = FALSE, do.osa = FALSE)
```

<br>

::: fragment

Para explorar los estimados de efectos fijos y su error estándar:

```{r eval = FALSE}
my_model$sdrep
```

:::

## Resultados

<br>

Dentro del elemento `$opt` tambien podemos encontrar información interesante:

```{r eval = FALSE}
my_model$opt$par # parámetros (efectos fijos) optimos estimados 
my_model$opt$objective # nll value (efectos fijos)
my_model$opt$convergence # 0 = model has converged
```

<br>

::: fragment

Para explorar el gradiente final de cada parámetro (efecto fijo):

```{r eval = FALSE}
my_model$final_gradient
```

:::

## Resultados

<br>

Para explorar los valores esperados, y la mayoría de variables obtenidas del modelo (e.g., SSB, captura esperada, composición por edad esperada, etc), lo vemos dentro de `$rep`:

```{r eval = FALSE}
my_model$rep$SSB # spawning biomass vector
my_model$rep$MAA # mortalidad natural a la edad y year
my_model$rep$LAA # talla media a la edad (jan1)
my_model$rep$pred_catch_paa # composicion por edades pesqueria predicha
```

Se recomienda explorar todos los elementos de salida.

## Resultados

<br>

Algo interesante es explorar también los elementos de `$rep` que tienen el character `_re`. Estos son las desviaciones para modelar variabilidad temporal en los diferentes componentes. Por ejemplo:

```{r eval = FALSE}
my_model$rep$WAA_re # desviaciones para peso medio nonparametric
my_model$rep$LAA_re # desviaciones para talla media nonparametric
my_model$rep$growth_re # desviaciones para talla media parametric
my_model$rep$M_re # desviaciones para mortalidad natural
```

# WHAM: Figuras

## Figuras

<br>

Existe una función principal para crear figuras y tablas a partir de un modelo en WHAM:

```{r eval = FALSE}
wham::plot_wham_output(mod = ..., # model object
                       dir.main = ..., # folder to save plots (character)
                       out.type = ...) # png or pdf 
```

# WHAM: Proyecciones

## Proyecciones

<br>

Es posible realizar proyecciones en WHAM utilizando la siguiente función:

```{r eval = FALSE}
wham::project_wham(model = ..., # model object
                   proj.opts = list(...)) # projection options 
```

## Proyecciones

<br>

Dentro de `proj.opts` (lista), debemos especificar lo siguiente:

* `n.yrs`: (integer) número de años de proyecciones
* `use.last.F`: (logical) Usar terminal F para proyecciones?
* `use.avg.F`: (logical) Usar average F (sobre ciertos años?) para proyecciones?
* `avg.yrs`: (vector) Años para promediar para `use.avg.F`

## Proyecciones

<br>

* `use.FXSPR`: (logical) Calcular y usar F at $X \%$ SPR para proyecciones?
* `use.FMSY`: (logical) Usar F at MSY para proyecciones?
* `proj.F`: (vector numeric) Especificar F para proyecciones. Longitud igual a `n.yrs`.
* `proj.catch`: (vector numeric) Especificar captura agregada para proyecciones. Longitud igual a `n.yrs`.

## Proyecciones

<br>

* `cont.ecov`: (logical) continuar Ecov process en proyecciones?
* `proj.ecov`: (matrix) especificar valores de Ecov para proyecciones.
* `cont.Mre`: (logical) continuar desviaciones en proyecciones?

Existen mismas opciones para crecimiento somático.

## Proyecciones

<br>

* `percentFXSPR`: (numeric) Solo usado cuando `use.FXSPR=TRUE`.
* `percentFMSY`: (numeric) Solo usado cuando `use.FMSY=TRUE`.

# Comentarios (personales) finales

## Opinión

::: incremental

- Muchos conceptos aprendidos en este curso pueden ser aplicado a otros SSM.

- Modelos de evaluación SSM estructurado por edad en TMB aún no son usados ampliamente.

- WHAM es una plataforma en desarrollo y muchas cosas cambiarán rápidamente.

- Grandes capacidades, y se espera una mayor la difusión de esta plataforma.

- Miren [FIMS](https://github.com/NOAA-FIMS) (el futuro?).

:::

## Sobre el curso

::: incremental

- Gracias por su participación!

- Horas de asesoría.

- Próximo curso: Introducción a la programación en R (25 al 29 septiembre, 15-18 horas).

- Déjanos una revisión en nuestro Facebook page.

:::

## Referencias
